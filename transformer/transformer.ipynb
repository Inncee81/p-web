{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f33e45",
   "metadata": {},
   "source": [
    "using example in https://huggingface.co/blog/how-to-train\n",
    "https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=QDNgPls7_l13\n",
    "\n",
    "\n",
    "work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daab1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a96d8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import tokenizers\n",
    "\n",
    "\n",
    "notebook_path = os.path.abspath(\"RoBERTa.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53c42548",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./../tokenize/vocab.json\",\n",
    "    \"./../tokenize/merges.txt\",\n",
    ")\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "# or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "examples = []\n",
    "\n",
    "src_file = os.path.realpath(\"../tokenize/pre-tokenized/modified_cnn.txt\")\n",
    "\n",
    "with open(src_file, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        try:\n",
    "            rawContent = f.read() \n",
    "        except Exception as e:\n",
    "            print('During file reading, this error occurred:', e)\n",
    "\n",
    "\n",
    "# tokenizer.encode(rawContent)\n",
    "tokenizer.encode(rawContent).tokens\n",
    "\n",
    "tokenizer.save(\"byte-level-BPE.tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad39b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb6e77d",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "84095008"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "048fff48",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'tokenizer'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b3b3aa282499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Debug through https://github.com/huggingface/transformers/issues/7234\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"byte-level-BPE.tokenizer.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'pad_token'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'[PAD]'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'tokenizer'"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "# Debug through https://github.com/huggingface/transformers/issues/7234\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"byte-level-BPE.tokenizer.json\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Just training on one point for debugging\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"../tokenize/pre-tokenized/modified_cnn.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60654df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./model_one\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./model_one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check if everything is working later?\n",
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"model_one\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "result = fill_mask(\"La suno <mask>.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd021c57f0bacba526f117c7a35a51dadbd205f58a019ce8860c369097327f4dc19",
   "display_name": "Python 3.8.5 64-bit ('p-web-sfGkgXUp': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}